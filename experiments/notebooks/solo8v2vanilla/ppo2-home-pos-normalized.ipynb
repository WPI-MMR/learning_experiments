{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO2 on Solo8 v2 Vanilla for Quadrupedal Standing w/ a Multiplicitive Reward & Full Normalization\n",
    "Try to get the solo to stand on 4 feet stabley. Normalized both the action and observation spaces to fall between $[-1, 1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure that Tensorflow is using the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Experiment Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGS = ['solov2vanilla', 'gpu', 'home_pos_mulitiplicitive', \n",
    "        'normalized_actions', 'normalized_observations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "from gym_solo.envs import solo8v2vanilla\n",
    "from gym_solo.core import obs\n",
    "from gym_solo.core import rewards\n",
    "from gym_solo.core import termination as terms\n",
    "\n",
    "import gym\n",
    "import gym_solo\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse CLI arguments and register w/ wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment will be using the auto trainer to handle all of the hyperparmeter running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from auto_trainer import params\n",
    "import auto_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give the robot a total of 10 seconds simulation time to learn how to stand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_length = 2 / solo8v2vanilla.Solo8VanillaConfig.dt\n",
    "episode_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a basic config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(algorithm='PPO2', episode_length=2000.0, episodes=4000, eval_episodes=3, eval_frequency=1, eval_render_freq=44, f='/root/.local/share/jupyter/runtime/kernel-58374ea9-9d4f-494d-98f5-118814f9bdd9.json', flat_reward_hard_margin=0.1, flat_reward_soft_margin=3.141592653589793, fps=15, height_reward_hard_margin=0.025, height_reward_soft_margin=0.15, height_reward_target=0.33698, hor_vel_reward_hard_margin=0.5, hor_vel_reward_soft_margin=3, max_motor_rotation=1.5707963267948966, num_workers=4, policy='MlpPolicy', small_control_reward_margin=10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = params.WandbParameters().parse()\n",
    "\n",
    "config.episodes = 4000\n",
    "config.episode_length = episode_length\n",
    "\n",
    "# Found experimentally - TODO make these into defaults\n",
    "config.max_motor_rotation = np.pi / 2\n",
    "config.flat_reward_hard_margin = 0.1\n",
    "config.flat_reward_soft_margin = np.pi\n",
    "config.height_reward_target = 0.33698\n",
    "config.height_reward_hard_margin = 0.025\n",
    "config.height_reward_soft_margin = 0.15\n",
    "config.small_control_reward_margin = 10\n",
    "config.hor_vel_reward_hard_margin = 0.5\n",
    "config.hor_vel_reward_soft_margin = 3\n",
    "\n",
    "config.num_workers = 4\n",
    "config.eval_frequency = 1\n",
    "config.eval_episodes = 3\n",
    "config.fps = 15\n",
    "\n",
    "# Create a 3 second gif\n",
    "config.eval_render_freq = int(config.episode_length / (3 * config.fps))\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33magupta231\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "/usr/local/lib/python3.7/dist-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.19<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">dark-glitter-309</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/wpi-mmr/solo-rl-experiments\" target=\"_blank\">https://wandb.ai/wpi-mmr/solo-rl-experiments</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/wpi-mmr/solo-rl-experiments/runs/1uyd0cpo\" target=\"_blank\">https://wandb.ai/wpi-mmr/solo-rl-experiments/runs/1uyd0cpo</a><br/>\n",
       "                Run data is saved locally in <code>/sources/learning_experiments/experiments/notebooks/solo8v2vanilla/wandb/run-20210220_025335-1uyd0cpo</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'episodes': 4000, 'episode_length': 2000.0, 'policy': 'MlpPolicy', 'algorithm': 'PPO2', 'num_workers': 4, 'eval_episodes': 3, 'eval_frequency': 1, 'eval_render_freq': 44, 'fps': 15, 'f': '/root/.local/share/jupyter/runtime/kernel-58374ea9-9d4f-494d-98f5-118814f9bdd9.json', 'max_motor_rotation': 1.5707963267948966, 'flat_reward_hard_margin': 0.1, 'flat_reward_soft_margin': 3.141592653589793, 'height_reward_target': 0.33698, 'height_reward_hard_margin': 0.025, 'height_reward_soft_margin': 0.15, 'small_control_reward_margin': 10, 'hor_vel_reward_hard_margin': 0.5, 'hor_vel_reward_soft_margin': 3}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config, run = auto_trainer.get_synced_config(config, TAGS)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the following inputs to the robot / environment:\n",
    "\n",
    "**Observations**\n",
    "- TorsoIMU\n",
    "- Motor encoder current values\n",
    "\n",
    "**Reward**\n",
    "- How flat the torso is :$f$\n",
    "- Minimize the amount of control in the joints: $c$\n",
    "- Minimize the amount of torso movement: $m$\n",
    "- Keeping the torso at a given height: $h$\n",
    "\n",
    "We'll compose the \"standing\" reward to be $\\frac{f + h}{2}$ as $f,h \\in [0, 1]$. Then, the final reward becomes:\n",
    "\n",
    "$$reward = \\frac{f + h}{2} cm$$\n",
    "\n",
    "Note that since $c,m \\in [0, 1]$, this enforces that $reward \\in [0, 1]$\n",
    "\n",
    "**Termination Criteria**\n",
    "- Terminate after $n$ timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(length, max_motor_rot, fhm, fsm, stand_height, thm, tsm, scm, hmhm, hmsm):\n",
    "    def _init():\n",
    "        env_config = solo8v2vanilla.Solo8VanillaConfig()\n",
    "        env_config.max_motor_rotation = max_motor_rot\n",
    "        env = gym.make('solo8vanilla-v0', config=env_config, \n",
    "                    normalize_actions=True, \n",
    "                    normalize_observations=True)\n",
    "\n",
    "        env.obs_factory.register_observation(obs.TorsoIMU(env.robot))\n",
    "        env.obs_factory.register_observation(\n",
    "            obs.MotorEncoder(env.robot, max_rotation=max_motor_rot))\n",
    "        env.termination_factory.register_termination(terms.TimeBasedTermination(length))\n",
    "        \n",
    "        stand_reward = rewards.AdditiveReward()\n",
    "        stand_reward.client = env.client\n",
    "        \n",
    "        stand_reward.add_term(0.5, rewards.FlatTorsoReward(\n",
    "            env.robot, hard_margin=fhm, soft_margin=fsm))\n",
    "        stand_reward.add_term(0.5, rewards.TorsoHeightReward(\n",
    "            env.robot, stand_height, hard_margin=thm, soft_margin=tsm))\n",
    "        \n",
    "        home_pos_reward = rewards.MultiplicitiveReward(1, stand_reward,\n",
    "            rewards.SmallControlReward(env.robot, margin=scm),\n",
    "            rewards.HorizontalMoveSpeedReward(env.robot, 0,\n",
    "                                            hard_margin=hmhm,\n",
    "                                            soft_margin=hmsm))\n",
    "        \n",
    "        env.reward_factory.register_reward(1, home_pos_reward)\n",
    "        return env\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Envs\n",
    "Import the desired vectorized env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines.common.vec_env import VecNormalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load config and create the environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = [make_env(config.episode_length, \n",
    "                config.max_motor_rotation,\n",
    "                config.flat_reward_hard_margin,\n",
    "                config.flat_reward_soft_margin,\n",
    "                config.height_reward_target,\n",
    "                config.height_reward_hard_margin,\n",
    "                config.height_reward_soft_margin,\n",
    "                config.small_control_reward_margin,\n",
    "                config.hor_vel_reward_hard_margin,\n",
    "                config.hor_vel_reward_soft_margin) \n",
    "        for _ in range(config.num_workers + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training & testing environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: DeprecatedWarning: register_reward is deprecated. This will probaby be removed for AdditiveReward down the line. For best future-proofing pratices, create an AdditiveReward and register it with weight 1\n"
     ]
    }
   ],
   "source": [
    "train_env = SubprocVecEnv(envs[:-1])\n",
    "test_env = envs[-1]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "And we're off!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/base_class.py:1169: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "-------------------------------------\n",
      "| approxkl           | 0.003917259  |\n",
      "| clipfrac           | 0.04345703   |\n",
      "| explained_variance | 0.129        |\n",
      "| fps                | 469          |\n",
      "| n_updates          | 1            |\n",
      "| policy_entropy     | 17.036047    |\n",
      "| policy_loss        | -0.013424931 |\n",
      "| serial_timesteps   | 128          |\n",
      "| time_elapsed       | 0.000504     |\n",
      "| total_timesteps    | 512          |\n",
      "| value_loss         | 1.3211926    |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Step cannot be set when using syncing with tensorboard. Please log your step values as a metric such as 'global_step'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:502: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, config, run = auto_trainer.train(train_env, test_env, config, TAGS, \n",
    "                                        log_freq=1000, full_logging=False, run=run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.3",
    "jupytext_version": "1.10.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
